---
title: "Data Analysis from a Microfinance RCT in India"
author: "Preksha Jain"
date: "3/11/2022"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/PJain/Documents/R_Sample/01 Data/")
```

## Dataset Description

From 2009 to 2016, a cluster randomized controlled trial was implemented by several researchers to study the impact of local banks’ 
expansion on households’ loans, savings, and insurance talking behavior. The research project sought to evaluate the impact of the 
intervention on a wide range of outcomes that reflect household well-being (e.g. household income, consumption), as well as individual 
well-being (e.g. women’s empowerment, health). The research implementation partner was a large financial institution in rural South India 
that randomly expanded bank infrastructure across rural villages (called service areas) in 3 districts of Tamil Nadu, India. In 2009, 101
service areas over three districts were identified, which formed 50 service area pairs. One service area “pair” is a triplet, containing 
one treatment area and two control area. 50 treatment areas and 51 control areas given a total of 101 service areas. A bank branch was 
assigned to each service area, and the bank opened branches in the treatment group service area at the time of assignment, while expansion
into the control group areas after 18-24 months later. More than 4,000 households were randomly selected across all service areas to be 
included in the study. The opening of bank branches happened in three rounds. Thus, there are three baseline surveys, and three endline surveys.

## Codebook

1. treatment_status.csv
    * pair_id: uniquely identifies a service area control and treatment pair
    * group_id: uniquely identifies one service area
    * treated: indicator ==1 denoting a member of the treatment group

2. endline.dta
    * hhid: unique identifier for each household
    * totformalborrow_24: total formal borrowed amount (loans) in India Rupees in the past 24 months
    * totinformalborrow_24: total informal borrowed amount (loans) in India Rupees in the past 24 months
    * hhinc: self-reported total household income in the last 30 days
    * survey_round: the round of the survey, either endline 1, 2 or 3.
    * hhnomembers: Number of household members in each household

3. baseline_controls.dta: This dataset contains baseline household demographics including gender, age, education of head of household, 
household religions and household caste.



## Description of Tasks Performed in this File

1. Data Preparation
      a. Load the endline data.
      b. Recode household debt and income variables as numeric values instead of strings, and 
      replace “None” with 0.
      c. Browse the variables in this dataset, and describe the financial status of 
      households in this sample, supported by this data
      d. Top code household debt and income variables, replacing all values greater than three 
      standard deviations above the mean with a value that is equal to three standard deviations
      above the mean.
      e. Label the new top coded variables.
      f. Explain why we might want to top code these types of survey 
      responses from households, and give an example of another data quality or cleaning check 
      you might want to implement in this type of data.
      g. Create a total borrowed amount variable that equals the sum of formal and informal 
      borrowed amounts.
      h. Merge the endline data with the treatment_status dataset to assign a treatment status for 
      each household.
      i. Create a below poverty line dummy using the World Bank poverty line of 1.90 USD 
      (equivalent to 26.995 rupees in 2010 PPP units) per day per capita. You will need to use the 
      total household income over the last 30 days to find the daily household income, and then 
      find the income per capita per day for that household. Label the new variable and note if 
      there are any missing values.
      j. Describe the strengths and limitations of using the dummy created to assess a household’s 
      poverty status. If you were able to collect more data from these households, what types of 
      additional questions might you ask?
      k. Merge your working data with the baseline controls dataset, and save the merged data (be 
      aware of dealing with households that are in baseline only, or endline only. State your 
      reasoning of handling this issue).


2. Analysis
      a. In a sentence or two, state a testable hypothesis about the possible effects of this program, 
      and justify your prior (or prediction) for this particular effect. 
      b. Choose a few baseline household variables, and perform t-tests or produce a balance table to 
      test for the significance of differences between the treatment and control groups. 
          i. Why did you choose these particular variables to test? 
          ii. What are the results of the test, and what can they tell us about the validity of the 
              experiment? 
          iii. Please present the t-test or balance check in a table.
      c. Regress (with OLS) the household income on the treatment dummy. Include pair fixed 
      effects, and correct standard errors if necessary. 
          i. Explain why you think it might be appropriate to use a fixed effects specification in 
          this case, and how you would interpret the effect of the treatment on household 
          income in this case. Explain the meaning both of the point estimate and of the 
          statistical significance
          ii. Briefly justify your choice of the standard errors
      d. Generate a log income variable, and re-run the previous specification with log household 
      income as the dependent variable. 
          i. What are the key differences between the results of this regression and the results of 
          the previous specification? 
      e. Re-run the previous regression including a set of household-level controls.
          i. Explain why you chose these controls, and if there are key differences in your results 
          compared to previous specifications.
          ii. Export and save a regression table suitable for publication from these results
      f. Create a bar chart suitable for publication that summarizes the average borrowed amount for each 
      income quartile, by treatment group.


## Data Preparation

```{r}
# Loading the library to convert .dta file to .csv
library(rio)

# Converting endline.dta to endline.csv
convert("endline.dta", "endline.csv")

# Reading endline data into R
endline <- read.csv("endline.csv", header = T)

# Browsing and understanding variables and data structure 
str(endline)
dim(endline)
summary(endline)
```



```{r}
# Replacing "None" with "0" in new debt and income variables, simultaneously changing variables' class to numeric
endline$new_totformbor_24 <- as.numeric(gsub("None", "0", endline$totformalborrow_24))
endline$newtotinformbor_24 <- as.numeric(gsub("None", "0", endline$totinformalborrow_24))
endline$new_hhinc <- as.numeric(gsub("None", "0", endline$hhinc))

# Checking dimensions and summary stats
dim(endline)
summary(endline)
```


```{r}
# Inferences about the financial status of households - checking if the mean/median income for HHs which borrow more formally 
# differs from those those borrow more informally 
summary(endline$new_hhinc[endline$new_totformbor_24 > endline$newtotinformbor_24])
summary(endline$new_hhinc[endline$new_totformbor_24 < endline$newtotinformbor_24])

summary(endline$new_totformbor_24[endline$new_totformbor_24 > endline$newtotinformbor_24])
summary(endline$newtotinformbor_24[endline$new_totformbor_24 < endline$newtotinformbor_24])

summary(endline$newtotinformbor_24[endline$new_totformbor_24 > endline$newtotinformbor_24])
summary(endline$new_totformbor_24[endline$new_totformbor_24 < endline$newtotinformbor_24])



# It seems preliminarily that there is indeed a difference between the mean income of households which borrow more formally vs. 
# informally, i.e. richer households can afford/have access to formal sources of lending as compared to poorer households which 
# have to rely on more informal sources of lending. It also seems like the ticket size for formal vs. informal borrowing is higher
# for those who borrow dominantly from the respective sources, however, not so much if we look at non-dominant sources. 
# These preliminary inferences can be made more concrete through a t-test.

```


```{r}
# Creating a function to replace outliers (values greater than three s.d.'s of the mean) with cutoff value -> Top coding
outlierReplace <- function(x){
  cutoff <- mean(x[!is.na(x)]) + 3*sqrt(var(x[!is.na(x)]))
  x[x>cutoff] <- cutoff
  return(x)
}



# Applying the function (top-coding) to debt and income variables
endline$new_hhinc <- outlierReplace(endline$new_hhinc)
endline$new_totformbor_24 <- outlierReplace(endline$new_totformbor_24)
endline$newtotinformbor_24 <- outlierReplace(endline$newtotinformbor_24)



# It is not possible to label variables in R so I have just replaced them
# It is important to top-code income and debt variables since we don't want outliers in the data to drive the treatment effects 
# and also reduce the precision of our estimates - it is to make the model more robust.
# Other checks could be to test for "good variation" in our data, and retain only those variables that satisfy a given criterion.
# Another could be to check for missing values and how to handle them.
```



```{r}
# Creating a new variable that captures total borrowed amount in the last 24 months (sum of formal and informal borrowing in the L24M)
endline$new_totbor_24 <- rowSums(cbind(endline$new_totformbor_24, endline$newtotinformbor_24), na.rm = T)
```



```{r}
# Loading the treatment status data into R
treatment_status <- read.csv("treatment_status.csv", header = T)

# Analyzing the data structure
str(treatment_status)

# Changing variable classes from integer to factor as appropriate
treatment_status[,c(1:3)] <- lapply(treatment_status[,c(1:3)], as.factor)

# Re-analyzing data structure after modifying variable classes
str(treatment_status)

# Verifying the information given in the question about 51 control groups and 50 treatment groups
table(treatment_status$treated)
```



```{r}
# Merging the treatment_status data with the endline data by using the common column "group_id"
endline_merged <- merge(endline, treatment_status, by = "group_id")



# Checking the data structure of the merged dataset
str(endline_merged)
head(endline_merged)
dim(endline_merged)
```

```{r}
# Creating a dummy variable for HHs with per capita daily income below the poverty line of Rs. 26.995 
# (2010 PPP conversion of USD 1.90). This dummy takes the value 1 if below poverty line, 0 if not. 
endline_merged$bpl <- as.factor(ifelse(endline_merged$new_hhinc/endline_merged$hhnomembers/30 < 26.995, 1, 0))



# There are 4 missing values reported for HHs which refused to answer the question on household income
endline_merged[is.na(endline_merged$bpl),]



# The strength of this dummy is that it helps identify poorest of the poor households using a global standard of a 
# poverty line, which is comparable across countries. However, the negative is that income might be misreported and the 
# distribution of income within the household might be unequal, as is the case in many developing countries where income 
# of males is often higher than females and children. If I could ask addditional questions, I would ask questions about 
# the household consumption and also individual consumption if possible because:
# a) the reporting is likely to be more accurate 
# b) individual level effects would become more pronounced. I would also ask about the seasonality of income because the
# staggered nature of income might cause acute poverty in certain months, inducing borrowing, which is measured over a longer period,
# causing disparity in comparison.
```

```{r}
# Reading baseline data into R by converting the .dta file to .csv
convert("baseline_controls.dta", "baseline.csv")
baseline <- read.csv("baseline.csv", header = T)



# Understanding the data structure and getting summary stats
str(baseline)
dim(baseline)
summary(baseline)



# Converting certain variable classes from integer to factor as appropriate and checking data structure again
baseline[,c(1,2,4,7,8,12:17)] <- lapply(baseline[,c(1,2,4,7,8,12:17)], as.factor)
str(baseline)
```



```{r}
# Getting data on households present in both baseline and endline surveys
common_end_base <- intersect(endline_merged$hhid, baseline$hhid)
length(common_end_base)


# There are 3,802 common households
```
```{r}
# Identifying which endline households are in the common dataset. w gives a true/false value based on occurrence in the common dataset or not
w <- endline_merged$hhid %in% common_end_base



# Creating another data frame which only contains households from the endline that are also present in baseline
endline_merged_2 <- data.frame(hhid = endline_merged$hhid[w])



# Checking dimensions and structure of the data frame - still have 3,802 observations  
dim(endline_merged_2)
head(endline_merged_2)



# Now, merging the new dataset with baseline, retaining both, common households and those which were present only in baseline, adding 
# both rows and columns
endline_merged_2 <- merge(endline_merged_2, baseline[, names(baseline)], all = T)



# Checking dimensions of dataset - now we have 4,066 households, indicating an addition of 264 households that were present in baseline 
# but not endline
dim(endline_merged_2)
```



```{r}
# Now combining the data with common and baseline-only households with endline-only households, but retaining only common values for the moment
eb_combined <- merge(endline_merged_2, endline_merged, by = c("hhid", "group_id", "hhnomembers"))




# Checking dimensions of the fully combined dataset. It seems a little off since the #households dropped from 3,802 to 3,800
dim(eb_combined)
names(eb_combined)
```

```{r}
# Trying to identify why 2 HHs dropped out -- seems like the group_id coding differs in endline and baseline for these 2 HHs
eb_combined <- merge(endline_merged_2, endline_merged, by = c("hhid", "hhnomembers"))



# Identifying and browsing the particular HHs that got dropped. We see that the group_id in baseline in 152 while in endline is 148.
eb_combined[eb_combined$group_id.x != eb_combined$group_id.y,]

table(baseline$group_id)



# It is clear that there is a coding error in one of the surveys. Assuming that the baseline categorization was correct, replacing 
# the mis-categorization in endline with baseline values.



endline_merged[endline_merged$hhid == 106131, 1] <- 152
endline_merged[endline_merged$hhid == 106360, 1] <- 152



# Now, creating the final merged dataset with endline-only, baseline-only, and common households
eb_combined <- merge(endline_merged_2, endline_merged, by = c("hhid", "group_id", "hhnomembers"), all = T)



# Checking row dimensions: sense-check the answer, which should be 4,066 + 4,160 - 3802 = 4,424. # Checking if all columns have 
# been included. Again, sense-check: answer should be 14 + 17 - 3 = 28. The answer is verified. Therefore, the data merging process 
# should have worked fine.
dim(eb_combined)



# Checking the data structure and vital signs.
names(eb_combined)
summary(eb_combined)
str(eb_combined)
```



```{r}
# To deal with baseline-only and endline-only households, we create a dummy to identify which is which. This dummy takes on the value 
# 0 if common between baseline and endline, 1 if baseline-only, and 2 if endline-only. 
eb_combined$missing_status <- ifelse(eb_combined$hhid %in% common_end_base, 0, ifelse(eb_combined$hhid %in% baseline$hhid, 1, 2))



# Checking how many values we end up with for each dummy - the answer ties in with the answers we have got previously so the process works.
table(eb_combined$missing_status)



# I have added dummy variables instead of dropping observations because it is important to not mess up the balance between the treatment 
# and control groups. Dummies allow one to analyze the difference, if any, between the characteristics of the three categories of households,
# i.e. common, endline-only, baseline-only.
```

## Data Analysis


```{r}
# The testable hypotheses could be whether 
# a) access to formal credit reduces informal lending and increases formal lending (expect formal borrowing to increase and informal borrowing
# to decrease) 
# b) access to more/better credit terms increases household income (expect hh income to increase) 
# c) savings respond to better credit terms (expect savings to increase)
```



```{r}
# Choosing the following variables to test because they are expected to have an impact on key outcome variables and hence we want to make 
# sure that the groups are 'balanced', i.e. they are not statistically significantly different from each other. This can be seen by the p-values
# of the following t-tests, all of which are >0.05, so we fail to reject that the two groups are significantly different from each other.
# This means our randomization is valid and so is our experiment and its conclusions.




# Demographics
t1 <- t.test(as.numeric(eb_combined$hhid) ~ eb_combined$treated)
t2 <- t.test(as.numeric(eb_combined$hhcaste_sc_st) ~ eb_combined$treated)
t3 <- t.test(as.numeric(eb_combined$hhcaste_fc) ~ eb_combined$treated)



# Income
t4 <- t.test(as.numeric(eb_combined$new_hhinc) ~ eb_combined$treated)
t5 <- t.test(as.numeric(eb_combined$bpl) ~ eb_combined$treated)



# Characteristics of the head of household
t6 <- t.test(as.numeric(eb_combined$gender_hoh) ~ eb_combined$treated)
t7 <- t.test(as.numeric(eb_combined$age_hoh) ~ eb_combined$treated)
t8 <- t.test(as.numeric(eb_combined$educyears_hoh) ~ eb_combined$treated)
t9 <- t.test(as.numeric(eb_combined$readwrite_hoh) ~ eb_combined$treated)
t10 <- t.test(as.numeric(eb_combined$noclasspassed_hoh) ~ eb_combined$treated)

t <- list(t1,t2,t3,t4,t5,t6,t7,t8,t9,t10)

t
# The estimates I got here show the balance across groups
```



```{r}
# Loading relevant libraries
library("clubSandwich") # helps test for coefficients by clustering standard errors
library("plm") # helps run fixed effects linear model



# Running OLS regressing household income on treatment dummy, with pair fixed effects
hh_inc_on_treatment <- lm(eb_combined$new_hhinc ~ eb_combined$treated + eb_combined$pair_id - 1)
summary(hh_inc_on_treatment)

plm_model <- plm(new_hhinc ~ treated, data = eb_combined[!is.na(eb_combined$new_hhinc),], index = c("pair_id"), model = "within")
summary(plm_model)



# It is appropriate to use a fixed effects specification here because we want to control for time-invariant characteristics at the 
# pair level and isolate the effects of the treatment. In this case, at a pair level, except for one pair, the pair fixed effects 
# are statistically significant at 0.1% level. This provides validity to our fixed effects specification, suggesting that pair-level 
# characteristics do explain the variation in HH income. 



# The point estimate is the difference between means of the treatment and control groups as given by the plm_model, which shows that 
# the difference in HH income between treatment and control groups is Rs. 715 and is not statistically different from zero,
# i.e. we fail to reject that the treatment caused a significant increase in HH income for the treated HHs.



# Testing coefficient after clustering standard errors at the group level -> corrected standard errors
coef_test(plm_model, vcov = "CR2", cluster = eb_combined$group_id, test = "Satterthwaite")



# It is reasonable for us to cluster standard errors at the group_id level because it represents a certain area for service delivery
# and we expect errors to be correlated within those areas. Even after correcting for standard errors, the treatment effect is not statistically significant.
```


```{r}
#Redefining a log(hhinc) variable
eb_combined$new_log_hhinc <- log(eb_combined$new_hhinc)



# Defining new data for which log(hhinc) is not NA or -Inf
new_data <- eb_combined[!is.na(eb_combined$new_log_hhinc) & eb_combined$new_log_hhinc > 0,]



#Checking dimensions of the new data
dim(new_data)

# Running a log specification with pair fixed effects :
log_hh_inc_on_treatment <- plm(new_log_hhinc ~ treated, data = new_data, index = "pair_id", model = "within")

summary(log_hh_inc_on_treatment)



# Running a log specification leads our coeeficient, that is treatment effect to be significant at 10% level. It brings down the 
# standard error comparitively and at 10% level, we can reject that there was no increase in HH income due to the treatment. 
# Since we have a smaller set of observations here, we are compromising a bit on the power of our test.
```



```{r}
# Re-running the previous specification with household-level controls (age, gender, education, caste, religion, members over 18 years of age)
log_hh_inc_on_treatment_controls <- plm(new_log_hhinc ~ treated + age_hoh + gender_hoh + educyears_hoh + hhcaste_sc_st + hhcaste_fc 
                                      + hhreg_muslim + hhnomembers_above18 + hhnomembers, 
                                      data = new_data, index = "pair_id", model = "within")

summary(log_hh_inc_on_treatment_controls)




# I chose these controls because they are intuitively likely to explain variation in hh income; the fact that almost all are significant
# shows that the treatment effect could have been biased earlier, due to the omitted variables bias, since we failed to account for key 
# factors that are correlated with both the treatment and the hh income. After including hh level controls, we find that our treatment 
# effects becomes significant at 5% level as the magnitude of our estimate increases.
```



```{r, eval=FALSE}
# Creating publication quality regression output in LaTeX

library("stargazer")

stargazer(log_hh_inc_on_treatment_controls, 
          title = "Regression Results with Household Level Controls", 
          dep.var.labels = c("Log of Household Income over last 30 days"), 
          covariate.labels = c("Treatment",
                            "Age (Head of Household)", 
                            "Gender (Head of Household)",
                            "Years of Education (Head of Household)",
                            "Caste - SC/ST", "Caste - Forward",
                            "Religion - Muslim",
                            "No. of Household members over age of 18",
                            "No. of Household members"))
```



```{r}
#Defining income quartiles
quantile(eb_combined$new_hhinc, c(0.25, 0.5, 0.75, 1), na.rm = T)

eb_combined$new_hhinc_quartile <- ifelse(eb_combined$new_hhinc < 2850, "I", 
                                  ifelse(eb_combined$new_hhinc < 6000, "II", 
                                  ifelse(eb_combined$new_hhinc < 11000, "III", 
                                  ifelse(eb_combined$new_hhinc <= 214190.3, "IV", NA))))



# Creating a data frame to plot the barchart
avg_borr_inc <- aggregate(eb_combined$new_totbor_24, 
                by = list(eb_combined$treated, eb_combined$new_hhinc_quartile), FUN = mean)
avg_borr_inc <- data.frame(Treatment = avg_borr_inc[[1]], IncomeQuartile = avg_borr_inc[[2]], AvgBorrowing = avg_borr_inc[[3]])




# Plotting barchart
library("ggplot2")

ggplot(avg_borr_inc, aes(IncomeQuartile, AvgBorrowing)) + 
  geom_bar(aes(fill = Treatment), stat = "identity", position = "dodge") +
  labs(x = "Income quartiles", y = "Avg. borrowing in the last 24 months (Rupees)", 
  title = "Avg. borrowed amount for each income quartile, by treatment group")
```
